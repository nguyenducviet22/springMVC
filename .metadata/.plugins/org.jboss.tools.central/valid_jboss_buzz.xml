<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Try Camel K in the Developer Sandbox for Red Hat OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/30/try-camel-k-developer-sandbox-red-hat-openshift" /><author><name>Bruno Meseguer</name></author><id>6b606d6a-f034-4db1-b7a6-a4fddb3e6c1c</id><updated>2023-03-30T07:01:00Z</updated><published>2023-03-30T07:01:00Z</published><summary type="html">&lt;p&gt;You can now try &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K&lt;/a&gt; in the &lt;a href="https://developers.redhat.com/developer-sandbox/"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, an &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; environment you can access for a free, hands-on experience in building and deploying cloud-native applications quickly. This article will guide you to the Developer Sandbox and through a Camel K integration in a fully web-based experience—no local installs needed.&lt;/p&gt; &lt;p&gt;If you are unfamiliar with Camel K, it is a subproject of Apache Camel, which many know as the Swiss Army knife of integration. Camel K simplifies the process of running cloud-native integration flows in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; environments.&lt;/p&gt; &lt;h2&gt;What’s so special about Camel K?&lt;/h2&gt; &lt;p&gt;Many organizations and developers implement &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; with varied languages and frameworks, but they usually forget the existence of purpose-built technologies such as Apache Camel, packed with hundreds of connectors and out-of-the-box patterns to resolve typical and challenging integration scenarios, such as content-based routing, splitting or data aggregation, protocol bridging, data transformation, and so on.&lt;/p&gt; &lt;p&gt;Camel K was designed to encapsulate the key concepts of running integrations on &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, providing a significant degree of automation to simplify the process of creating, building, deploying, and operating integration flows in Kubernetes environments.&lt;/p&gt; &lt;p&gt;Usually, applications are defined in a complex project tree and include dependency descriptors to incorporate libraries necessary to run the application. Camel K, on the other hand, aims to simplify the project to let the developer focus on the process flow definition. It automatically analyzes the code to find needed dependencies and only requires the essential flow definitions and resources from the developer.&lt;/p&gt; &lt;h2&gt;Access the Developer Sandbox&lt;/h2&gt; &lt;p&gt;Follow these instructions to get started in the Developer Sandbox: &lt;a href="https://developers.redhat.com/articles/2023/03/09/how-access-developer-sandbox-red-hat-openshift#" target="_blank"&gt;How to access the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Once you have your browser connected to the Developer Sandbox console, you’ll be all set to start the first part of this article’s tutorial.&lt;/p&gt; &lt;h2&gt;Part 1: Roll the dice&lt;/h2&gt; &lt;p&gt;The most straightforward way to get started with Camel K is from the Developer view. Follow the instructions below to deploy a “Hello world” example.&lt;/p&gt; &lt;p&gt;From the Developer view, click &lt;strong&gt;+Add&lt;/strong&gt; in the left menu, and scroll down to the Developer Catalog to find the &lt;strong&gt;Operator Backed&lt;/strong&gt; category, as shown in Figure 1.&lt;/p&gt; &lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="add operator" data-entity-type="file" data-entity-uuid="b4a63ecc-9494-4979-a1d6-9ae539765b2f" height="577" src="https://developers.redhat.com/sites/default/files/inline-images/08-add-operator-backed.jpg" width="431" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Add 'Operator Backed' from the Developer Catalog.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;Locate the &lt;strong&gt;Integration&lt;/strong&gt; resource in the catalog, as illustrated in Figure 2:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Use the filter (type “integration”)&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Integration &lt;/strong&gt;tile, then click the blue &lt;strong&gt;Create&lt;/strong&gt; button that appears from the right.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="select integration" data-entity-type="file" data-entity-uuid="c31ad4a5-5acf-4e59-ad5f-f2445573bef5" src="https://developers.redhat.com/sites/default/files/inline-images/09-select-integration.jpg" width="1000" height="649" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Find the Integration resource.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;Click on the &lt;strong&gt;Configure via YAML view&lt;/strong&gt; radio button. You will then be presented with a definition screen similar to the one shown in Figure 3.&lt;/p&gt; &lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="YAML view" data-entity-type="file" data-entity-uuid="0c9c01eb-0d48-42c2-bc61-eeb23a0e278e" src="https://developers.redhat.com/sites/default/files/inline-images/10-create-integration.png" width="1000" height="1602" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: YAML view.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;By selecting the &lt;strong&gt;YAML view&lt;/strong&gt;, the interface lets you edit the definition and create the integration directly on the screen. This is a manual procedure that is very helpful when playing with the technology for the first time.&lt;/p&gt; &lt;p&gt;The default Camel route definition is very simplistic; let’s make it a bit more interesting so that you can start experiencing the Camel K operator in action.&lt;/p&gt; &lt;p&gt;Replace the default Camel route (&lt;code&gt;from&lt;/code&gt; definition) with the following snippet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; - from: uri: 'platform-http:/roll-dice' steps: - set-body: simple: 'roll: ${random(1,6)}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the preceding code, the &lt;code&gt;from&lt;/code&gt; element exposes an HTTP entry point for a service called &lt;code&gt;roll-dice&lt;/code&gt; that will serve random numbers from 1 to 6 when clients submit HTTP requests.&lt;/p&gt; &lt;p&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;When you create an integration as just described, the definition gets stored in your OpenShift project (namespace). Behind the scenes, the Camel K operator, deployed elsewhere, has visibility and picks up your definition to trigger a build and deployment on your project. This process takes some time.&lt;/p&gt; &lt;p&gt;To monitor the state of the integration deployment, open a terminal from where you can obtain information from the environment via the command line. Click the terminal icon at the top of your screen and then click the &lt;strong&gt;Start&lt;/strong&gt; button below to initialize the terminal, as shown in Figure 4.&lt;/p&gt; &lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Open the terminal for Camel K" data-entity-type="file" data-entity-uuid="9166decd-d6e5-4c74-8a7d-e09488d53336" src="https://developers.redhat.com/sites/default/files/inline-images/11-open-terminal.jpg" width="1000" height="910" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: Initialize the terminal.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;When you start the terminal, a new pod deploys. This is where your terminal is running.&lt;/p&gt; &lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Work terminal" data-entity-type="file" data-entity-uuid="a77e2973-d6c0-495b-8741-3df6af2f59d8" src="https://developers.redhat.com/sites/default/files/inline-images/12-work-terminal.jpg" width="1000" height="941" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5: Terminal running.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;On your terminal, type:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apache"&gt;oc get integration -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The flag &lt;code&gt;-w&lt;/code&gt; indicates to watch the resource for changes.&lt;/p&gt; &lt;p&gt;As the work progresses, its state will transition from &lt;code&gt;Building Kit&lt;/code&gt; to &lt;code&gt;Deploying&lt;/code&gt; to &lt;code&gt;Running&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You should see an output similar to the following:&lt;/p&gt; &lt;pre&gt; bash-4.4 ~ $ oc get integration -w NAME PHASE KIT REPLICAS example Building Kit kit-cdl6pa788ih7pmdvn6c0 example Deploying kit-cdl6pa788ih7pmdvn6c0 example Running kit-cdl6pa788ih7pmdvn6c0 0 example Running kit-cdl6pa788ih7pmdvn6c0 1 example Running kit-cdl6pa788ih7pmdvn6c0 1 example Running kit-cdl6pa788ih7pmdvn6c0 1&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;When the deployment completes, a new pod will run your integration definition:&lt;/p&gt; &lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Pod running the example" data-entity-type="file" data-entity-uuid="a36b66c3-65f3-48ca-9c2d-25ba4f3d738f" height="249" src="https://developers.redhat.com/sites/default/files/inline-images/13-hello-world.png" width="430" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6: Pod running the example.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;What’s interesting in this deployment is that the Camel K operator has the intelligence to recognize you’re exposing an HTTP endpoint and automatically deploys a Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank"&gt;service&lt;/a&gt; named &lt;code&gt;example&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To test the service from within OpenShift, you can invoke it from the terminal. Enter the following cURL command and execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;curl http://example/roll-dice&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command should return the value produced by the Camel integration. Executing it multiple times will produce random numbers from 1-6, simulating the dice rolling each time.&lt;/p&gt; &lt;p&gt;By default, the Developer Sandbox does not expose the service automatically to external consumers. If you want the service to be externally accessible, you can simply enable an OpenShift &lt;a href="https://docs.openshift.com/container-platform/4.11/networking/routes/route-configuration.html target="&gt;route&lt;/a&gt; by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apache"&gt;oc expose service example --path=/roll-dice&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command above uses the OpenShift client &lt;code&gt;oc&lt;/code&gt; to create a route that points to your new &lt;em&gt;Camel&lt;/em&gt; example service, to expose to the external world.&lt;/p&gt; &lt;p&gt;Now you can also call the &lt;code&gt;roll-dice&lt;/code&gt; service from your browser as if you were an external consumer. Notice the little icon attached to the Camel’s pod round graphic; click on it.&lt;/p&gt; &lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Camel K pod" data-entity-type="file" data-entity-uuid="9faa004d-5d6a-48b4-a55d-8e2e253d1a92" height="242" src="https://developers.redhat.com/sites/default/files/inline-images/14-hello-world-test.png" width="220" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 7: Camel example service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;If your browser renders "Resource not found" or another error message, make sure your address bar uses &lt;code&gt;http&lt;/code&gt; (not HTTPS), and include the service path &lt;code&gt;/roll-dice&lt;/code&gt; at the end of the URL, similar to:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;&lt;code&gt;http://&lt;/code&gt;&lt;/strong&gt;...&lt;long address value here&gt;...&lt;strong&gt;&lt;code&gt;/roll-dice&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In your browser, it should look similar to Figure 8.&lt;/p&gt; &lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="browser roll dice" data-entity-type="file" data-entity-uuid="d19d0215-234a-405e-b7b9-12a7583edfca" src="https://developers.redhat.com/sites/default/files/inline-images/15-hello-world-test-browser.jpg" width="1007" height="239" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 8: The browser.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;If you click the &lt;strong&gt;Reload &lt;/strong&gt;button several times, you’ll see different values displayed.&lt;/p&gt; &lt;p&gt;When you’re done, to clean up your working project, delete your integration with the simple command shown below:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apache"&gt;oc delete integration example&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You also need to delete the route you manually created by executing:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apache"&gt;oc delete route example&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Part 2: Inside OpenShift Dev Spaces&lt;/h2&gt; &lt;p&gt;If you want to play with Camel K in a more traditional developer workflow using a code editor and a terminal, the Developer Sandbox ships with an entire web-based IDE called &lt;a href="https://developers.redhat.com/products/openshift-dev-spaces/overview target="&gt;Red Hat OpenShift Dev Spaces&lt;/a&gt; (formerly Red Hat CodeReady Workspaces).&lt;/p&gt; &lt;h3&gt;Set up your dev environment with the Camel tutorials&lt;/h3&gt; &lt;p&gt;The animated sequence in Figure 9 illustrates the actions to follow to open your development environment along with your tutorial instructions.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/open-tutorials.gif"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/open-tutorials.gif" width="985" height="490" alt="gif to open dev tutorial" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: The Dev Spaces UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;From the web console, click the &lt;strong&gt;Applications&lt;/strong&gt; icon as shown in Figure 9 (marked 1).&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;Red Hat OpenShift Dev Spaces&lt;/strong&gt; (2).&lt;br /&gt; You will be prompted to log in and Authorize Access; select the "Allow selected permissions" option.&lt;/li&gt; &lt;li aria-level="1"&gt; &lt;p&gt;When the Create Workspace dashboard in OpenShift Dev Spaces opens, copy the snippet below:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-http"&gt;https://github.com/RedHat-Middleware-Workshops/devsandbox-camel.git&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, paste it into the &lt;strong&gt;Git Repo URL&lt;/strong&gt; field (3).&lt;/p&gt; &lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Create &amp; Open&lt;/strong&gt; (4).&lt;/li&gt; &lt;li aria-level="1"&gt;When the workspace finishes provisioning and the IDE opens, click the deployable Endpoints accordion (5).&lt;/li&gt; &lt;li aria-level="1"&gt;Then, click on the icon (6), which opens the tutorial in a new browser tab.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose the tutorial indicated in the next section.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Start the Camel K tutorial&lt;/h3&gt; &lt;p&gt;Select the &lt;strong&gt;Camel K - User Demo &lt;/strong&gt;tile, highlighted in Figure 10.&lt;/p&gt; &lt;p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Camel K tutorial" data-entity-type="file" data-entity-uuid="45527a98-ac86-4d78-b31b-f647d1ab83c2" height="497" src="https://developers.redhat.com/sites/default/files/inline-images/20-solution-explorer.jpg" width="755" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 10: Locating the Camel K tutorial.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;When you click on the tile, the Solution Explorer will show the lab introduction and the exercise chapters included, which you should be able to complete in around 15 minutes.&lt;/p&gt; &lt;p&gt;The aim of this use case demo is to help you get started with the basics of Camel K and play in the Developer Sandbox. For that reason, the use case selected is simple and friendly. The sequence diagram in Figure 11 illustrates the flow you’re about to create; you’ll get to see for yourself how little effort is required to complete it.&lt;/p&gt; &lt;p&gt; &lt;figure class="text-align-center align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="sequence diagram" data-entity-type="file" data-entity-uuid="fd692789-8aff-4eda-86d1-ce326bb75854" src="https://developers.redhat.com/sites/default/files/inline-images/21-seq-diagram.jpg" width="441" height="432" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 11: The Camel K flow.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt; &lt;p&gt;Enjoy the Camel ride!&lt;/p&gt; &lt;h2&gt;More Apache Camel resources&lt;/h2&gt; &lt;p&gt;This article ends here, but this should only be the start of your journey with Apache Camel. The Developer Sandbox for Red Hat OpenShift gives you the opportunity to play on a Kubernetes-based application platform with an integrated developer IDE (OpenShift Dev Spaces).&lt;/p&gt; &lt;p&gt;With your browser alone, you can quickly complete the Camel K lab and see by yourself how simply Camel resolves a typical use case and how easy it is to test, containerize, and run applications in OpenShift.&lt;/p&gt; &lt;p&gt;Camel K is a convenient way to build and deploy integrations with Apache Camel. Its simplicity and ease of use accelerate developers. You might, however, find other Camel runtimes better suited for more advanced use cases. We encourage you to check out the resources below to learn more about Camel K and explore different ways you can build applications with Apache Camel:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Play with more &lt;a href="https://developers.redhat.com/articles/2023/03/09/accelerate-your-cloud-native-learning-access-red-hat-developer-sandbox"&gt;tutorials in the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;A good place to start learning about Camel K is the &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K topic page&lt;/a&gt; on Red Hat Developer.&lt;/li&gt; &lt;li aria-level="1"&gt;Learn how to implement a complete API integration using &lt;a href="https://developers.redhat.com/articles/2021/11/24/normalize-web-services-camel-k-and-atlasmap-part-1"&gt;Camel K and AtlasMap&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Learn more about the different Camel runtimes available by reading &lt;a href="https://developers.redhat.com/articles/2022/03/14/choose-best-camel-your-integration-ride-part-1"&gt;Choose the best Camel for your integration ride&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Read &lt;a href="https://developers.redhat.com/articles/2021/12/06/boost-apache-camel-performance-quarkus"&gt;Boost Apache Camel performance with Quarkus&lt;/a&gt; for a detailed look at Camel Quarkus.&lt;/li&gt; &lt;li aria-level="1"&gt;Visit the &lt;a href="https://developers.redhat.com/products/integration/overview"&gt;Red Hat Integration&lt;/a&gt; page on developers.redhat.com to see complementary capabilities around Camel.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/30/try-camel-k-developer-sandbox-red-hat-openshift" title="Try Camel K in the Developer Sandbox for Red Hat OpenShift"&gt;Try Camel K in the Developer Sandbox for Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bruno Meseguer</dc:creator><dc:date>2023-03-30T07:01:00Z</dc:date></entry><entry><title>How to access the Developer Sandbox for Red Hat OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/30/how-access-developer-sandbox-red-hat-openshift" /><author><name>Bruno Meseguer</name></author><id>b582eb5d-7821-4b58-b5ad-b10477247ebe</id><updated>2023-03-30T07:00:00Z</updated><published>2023-03-30T07:00:00Z</published><summary type="html">&lt;p&gt;Red Hat Developer has many labs that you can easily access through your web browser. The &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; lets you experiment with building and deploying cloud-native applications in a real &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; environment using only your web browser.&lt;/p&gt; &lt;p&gt;With an integrated development environment (IDE) called &lt;a href="https://developers.redhat.com/products/openshift-dev-spaces/overview"&gt;Red Hat OpenShift Dev Spaces&lt;/a&gt; (formerly Red Hat CodeReady Workspaces), the Developer Sandbox is a playground for developers looking to explore &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;-based application platforms. This article provides a step-by-step guide to getting started with the Developer Sandbox, where you can, for example, dive into the process of creating a Camel integration through a web-based interface.&lt;/p&gt; &lt;p&gt;The best part? No local installs are required, making it easier than ever to get started. The steps in this article will guide you through how to set up your own free-to-use environment.&lt;/p&gt; &lt;h2&gt;Access the Developer Sandbox for Red Hat OpenShift&lt;/h2&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Navigate to the &lt;a href="https://developers.redhat.com/developer-sandbox" target="_blank"&gt;Developer Sandbox&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt; &lt;p&gt;Click the red button labeled &lt;strong&gt;Start your sandbox for free&lt;/strong&gt; (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Open developer sandbox" data-entity-type="file" data-entity-uuid="2b964245-2020-4e97-be20-0fa131cb5944" height="201" src="https://developers.redhat.com/sites/default/files/inline-images/01-try-devsandbox-comp.jpeg" width="577" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Start your sandbox for free.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li aria-level="1"&gt; &lt;p&gt;You will then be prompted to log in with your Red Hat account (Figure 2). If you don't have an existing account, click &lt;strong&gt;Register for a Red Hat account&lt;/strong&gt;. Complete the registration process to create an account and then return to this step.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The Developer Sandbox for Red Hat OpenShift login screen." data-entity-type=" file" data-entity-uuid="21d68975-5b2a-4f7a-9365-b4c266579238" height="214" src="https://developers.redhat.com/sites/default/files/inline-images/02-register-redhat-account-comp.jpeg" width="274" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Log in with your Red Hat account.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li aria-level="1"&gt; &lt;p&gt;You should see a screen, as shown in Figure 3. Click the red button: &lt;strong&gt;Start using your sandbox&lt;/strong&gt;. This will take you to the Red Hat OpenShift Dedicated login page.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Start using Sandbox" data-entity-type="file" data-entity-uuid="3a7fcbfb-7b8a-4d9e-97b3-b26eddf74522" height="266" src="https://developers.redhat.com/sites/default/files/inline-images/04-start-using-sandbox-comp.jpeg" width="568" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Start using the Sandbox.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li aria-level="1"&gt; &lt;p&gt;Click the &lt;strong&gt;DevSandbox&lt;/strong&gt; button from this screen (Figure 4).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Login with Dev sandbox" data-entity-type="file" data-entity-uuid="07bdd516-1067-4293-acba-213962ff7e18" height="152" src="https://developers.redhat.com/sites/default/files/inline-images/05-login-sandbox-comp.jpeg" width="562" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: Log in with Dev Sandbox&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li aria-level="1"&gt; &lt;p&gt;The environment will welcome you with the message shown in Figure 5.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Developer Sandbox Welcome" data-entity-type="file" data-entity-uuid="ead9f98c-7d71-44c8-8f46-3e1aceee1499" height="176" src="https://developers.redhat.com/sites/default/files/inline-images/07-welcome-developer-comp.jpeg" width="488" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5: Welcome to the Developer Perspective&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;You have now entered the Developer Sandbox!&lt;/p&gt; &lt;h2&gt;Activities to learn Camel and Camel K&lt;/h2&gt; &lt;p&gt;For developers looking to get started with Camel and Camel K, here are two step-by-step guides you can try:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/03/09/try-camel-k-developer-sandbox"&gt;Try Camel K in the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/02/10/how-run-camel-spring-boot-red-hat-developer-sandbox"&gt;How to run Camel on Spring Boot in the Developer Sandbox&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/30/how-access-developer-sandbox-red-hat-openshift" title="How to access the Developer Sandbox for Red Hat OpenShift"&gt;How to access the Developer Sandbox for Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bruno Meseguer</dc:creator><dc:date>2023-03-30T07:00:00Z</dc:date></entry><entry><title type="html">WildFly 28 Beta1 is released!</title><link rel="alternate" href="https://wildfly.org//news/2023/03/30/WildFly28-Beta-Released/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2023/03/30/WildFly28-Beta-Released/</id><updated>2023-03-30T00:00:00Z</updated><content type="html">I’m pleased to announce that the new WildFly and WildFly Preview 28.0.0.Beta1 releases are available for download at . NEW AND NOTABLE Observability The biggest changes in WildFly 28 Beta1 relate to the observability space. * The micrometer subsystem has been , bringing support. As part of this work, we’ve added support for . The micrometer subsystem was first introduced in WildFly Preview in WildFly 27. Note that the subsystem has been updated from what was in WildFly Preview 27 to switch to pushing metric data via OTLP to a remote collector, instead of supporting polling of data on the WildFly server’s management interface. (Server and JVM metrics can still be pulled from the management endpoint if the is configured.) * We’ve also added support for via a . * We’ve removed support for MicroProfile Metrics, except for a that’s been kept to facilitate configuration migration. MicroProfile Metrics users are encouraged to use the new micrometer subsystem. * We’ve removed support for MicroProfile OpenTracing, except for a that’s been kept to facilitate configuration migration. MicroProfile OpenTracing users are encouraged to use the new microprofile-telemetry-smallrye subsystem, or the opentelemetry subsystem upon which it is based. MicroProfile Besides the changes in the observability space noted above, there are a couple of important changes in WildFly 28’s MicroProfile support: * We’ve for via new microprofile-lra-coordinator and microprofile-lra-participant subsystems. * Except for MicroProfile Metrics and OpenTracing, which have been removed, we’ve updated our support for the other MicroProfile Platform specifications to the versions. Provisioning * We’ve added a new to make it easy to provision a server based on the new introduced in EE 10. * Related to this we’ve introduced new and Galleon layers. These layers allow a more tailored configuration compared to the existing ee and web-server layers. Other Treats * The clustering team has added support for . * The clustering and web teams have added support for . * The RESTEasy team has added support for . * The messaging-activemq subsystem now supports . * When you use OIDC, the security team has added support for . * The web team has added for Undertow listeners. * We’ve updated Hibernate ORM from the ORM 6.1 release to 6.2.0.CR4. RELEASE NOTES The full release notes for the release are in the . Issues fixed in the numerous underlying WildFly Core 20.0 beta releases are listed in the . Please try it out and give us your feedback, while we get to work on WildFly 28 Final! Our goal is for that to be available in mid April. Best regards, Brian</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title>The developer's guide to Red Hat Summit 2023</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/29/developers-guide-red-hat-summit-2023" /><author><name>Colleen Lobner</name></author><id>09e73bae-a9db-4655-98fc-0f1418d315b8</id><updated>2023-03-29T14:00:00Z</updated><published>2023-03-29T14:00:00Z</published><summary type="html">&lt;p&gt;The wait is over: The session catalog and agenda builder are now available for &lt;a href="https://www.redhat.com/en/summit"&gt;Red Hat Summit 2023&lt;/a&gt;, which kicks off May 23 in Boston, Massachusetts. This year, Summit will share the stage with AnsibleFest, bringing you the latest in &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt;, open hybrid cloud, and more, all in one place.&lt;/p&gt; &lt;p&gt;We’re also offering a virtual option so attendees can get the most out of their 3-day conference experience. Pre-register at no cost to view keynotes and select on-site sessions to watch at your own pace after the event concludes.&lt;/p&gt; &lt;p&gt;We’ve rounded up some of the highlights for developers to help you plan your agenda. Head over to the Red Hat Summit website to &lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023?tab.day=20230522"&gt;browse the full session catalog&lt;/a&gt; and &lt;a href="https://reg.experiences.redhat.com/flow/redhat/sum23/regGenAttendee/login?intcmp=7013a000003DMkYAAW"&gt;register today&lt;/a&gt;! &lt;/p&gt; &lt;h2&gt;Why attend Red Hat Summit?&lt;/h2&gt; &lt;p&gt;Here’s what Red Hat Summit has to offer:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Engage in hands-on learning opportunities.&lt;/strong&gt; Get started with a new tool and develop in-demand skills. Stop by the Red Hat Developer booth to explore what’s possible in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Find out how industry leaders are tackling today’s biggest challenges.&lt;/strong&gt; Get insights and learn from tech experts, Red Hat customers and partners, and community members via keynotes, interactive sessions, and networking.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Collaborate, network, and discover the latest Red Hat technologies.&lt;/strong&gt; You’ll walk away brimming with fresh ideas and inspiration to advance your projects, organization, and career.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Dive into developer experience&lt;/h2&gt; &lt;p&gt;If you’re passionate about good developer experience, here are a few workshops and interactive sessions we think you’ll love.&lt;/p&gt; &lt;h3&gt;Red Hat Developer Tools overview and roadmap&lt;/h3&gt; &lt;p&gt;This session explores how Red Hat supports a cloud-first approach to cloud-native development and how it relates to the multicloud and hybrid cloud challenges developers face today. &lt;/p&gt; &lt;p&gt;Red Hat Developer’s &lt;a href="https://developers.redhat.com/author/mithun-t-dhar/"&gt;Mithun Dhar&lt;/a&gt;, &lt;a href="https://developers.redhat.com/author/parag-dave"&gt;Parag Dave&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/author/mohit-suman/"&gt;Mohit Suman&lt;/a&gt; will illustrate how Red Hat's &lt;a href="https://developers.redhat.com/topics/developer-tools"&gt;developer tools&lt;/a&gt; remove friction and help teams ship applications and deliver value in less time and for less cost. They’ll cover:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The full life cycle of the development process, from creating a new application, iterating it, and supporting a security-focused software delivery pipeline.&lt;/li&gt; &lt;li aria-level="1"&gt;Developer productivity with &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Tools that meet developers where they are: &lt;a href="https://developers.redhat.com/products/openshift-ide-extensions/overview"&gt;integrated development environments&lt;/a&gt; (e.g., VS Code, IntelliJ, hosted), command-line interfaces (CLI), web UIs, and more.&lt;/li&gt; &lt;li aria-level="1"&gt;Hosted offerings that provide a frictionless experience for developers to experiment and iterate over their cloud-native applications.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1672958649092001ePts?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;The internal developer platform: Scaffolding for cloud-native development&lt;/h3&gt; &lt;p&gt;The Red Hat Developer team is excited to host a hands-on workshop focused on the internal developer platform (IDP) on Tuesday, May 23, from 6-8 pm ET. In this hands-on session led by Red Hat’s &lt;a href="https://developers.redhat.com/authors/ryan-jarvinen/"&gt;Ryan Jarvinen&lt;/a&gt;, attendees will explore how &lt;a href="https://developers.redhat.com/articles/2022/10/24/red-hat-joins-backstageio-community"&gt;Backstage&lt;/a&gt; encapsulates tools, services, documentation, and best practices in “golden paths” to ease onboarding and daily development work.&lt;/p&gt; &lt;p&gt;Each participant will build and deploy a 3-tier web application using headline Red Hat products and technologies. You will see how these tools work together to support daily development activities and construct flexible foundations for cloud-native development. &lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Discover how Backstage tools help you mount your team’s scaffolding and apply best practices atop.&lt;/li&gt; &lt;li aria-level="1"&gt;View and change source code in &lt;a href="https://developers.redhat.com/products/openshift-dev-spaces/overview"&gt;Red Hat OpenShift Dev Spaces&lt;/a&gt;, a web-based integrated development environment built on VS Code.&lt;/li&gt; &lt;li aria-level="1"&gt;Automate the build-test-deploy cycle using &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; and build-test pipeline modules as you iterate your application with new features and fixes.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1679078902891001Yxy9?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Shaping the future of the IDPs using Backstage&lt;/h3&gt; &lt;p&gt;Developers deserve great experiences, too! A developer portal can decrease the cognitive load for developers and provide solutions that help onboard developers as well as applications, resulting in faster delivery for less money. &lt;/p&gt; &lt;p&gt;In this session, Red Hat’s &lt;a href="https://developers.redhat.com/author/serena-chechile-nichols"&gt;Serena Chechile&lt;/a&gt; will demonstrate how Red Hat supports an internal developer platform (IDP) that solves challenges that developers face today while increasing engineering productivity. You’ll also learn about our developer productivity bundle for Backstage. &lt;/p&gt; &lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1677711290856001s4aC?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Supercharge your developer portal with Red Hat integrations&lt;/h3&gt; &lt;p&gt;Explore tools that help lower the barrier of entry for getting started with Backstage, providing a richer software catalog through a variety of plug-ins and integrations. In this session, Red Hat’s &lt;a href="https://developers.redhat.com/author/andrew-block/"&gt;Andrew Block&lt;/a&gt; and Tom Coufal will introduce these extension points to the Backstage ecosystem that provide integrations into various Red Hat offerings, starting with Red Hat’s single sign-on technology, Red Hat Quay, and Red Hat Advanced Cluster Management for Kubernetes. &lt;/p&gt; &lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1673460487820001sB3k?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Dev Spaces on ROSA: Enabling developers in a regulated enterprise environment &lt;/h3&gt; &lt;p&gt;A major UK bank needed a developer environment that could start up quickly, meet security standards for source code, integrate with different developer tooling, and serve a developer population at scale. They decided to spin up &lt;a href="https://developers.redhat.com/products/openshift-dev-spaces/overview"&gt;Red Hat OpenShift Dev Spaces&lt;/a&gt; on Red Hat OpenShift Service on AWS (ROSA).&lt;/p&gt; &lt;p&gt;In this session, NatWest Group’s Baljinder Kang and Red Hat’s &lt;a href="https://developers.redhat.com/author/anton-giertli/"&gt;Anton Giertli&lt;/a&gt; will explain how they used OpenShift Dev Spaces on ROSA and integrated developer tooling. This session will be useful for organizations that want to streamline their developer journeys while keeping the administration loads to a minimum.&lt;/p&gt; &lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1673455911314001CJJi?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p class="Indent2"&gt;&lt;strong&gt;&lt;em&gt;[ Learning path: &lt;a href="https://developers.redhat.com/node/279091"&gt;How to deploy an application using Red Hat OpenShift Service on AWS&lt;/a&gt; ]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h3&gt;Investing in developer experience at Delta Air Lines to fly higher and faster&lt;/h3&gt; &lt;p&gt;This session will examine how Delta Air Lines uses Red Hat technologies to create a better developer experience with a platform that meets strict internal requirements and the needs of a demanding industry, while still providing the autonomy that enables developers to be their most effective.&lt;/p&gt; &lt;p&gt;Red Hat’s Prakriti Koller, Eric Sauer, and Delta’s Brittany Doncaster will share:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;How they went about selecting measures that focused on people and business outcomes and committed to transparency along the way&lt;/li&gt; &lt;li aria-level="1"&gt;How using OpenShift Dev Spaces helped improve Delta’s speed to market&lt;/li&gt; &lt;li aria-level="1"&gt;Which working practices showed results&lt;/li&gt; &lt;li aria-level="1"&gt;What they learned while trying to collect and automate measurement&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1673483793804001yZt2?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;More hands-on labs and workshops&lt;/h2&gt; &lt;p&gt;These sessions dive into GitOps, Kubernetes automation, &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;, and other topics to help you stay ahead of the curve.&lt;/p&gt; &lt;h3&gt;Using GitOps to control and customize your OpenShift clusters and applications&lt;/h3&gt; &lt;p&gt;Based on the Argo CD project, Red Hat OpenShift GitOps enables organizations to implement GitOps-based &lt;a href="http://developers.redhat.com/topics/ci-cd"&gt;continuous delivery (CD)&lt;/a&gt; workflows to manage their OpenShift Container Platform instances and applications across multi-cluster Kubernetes environments. &lt;/p&gt; &lt;p&gt;In this session, you will explore the benefits of GitOps and where to begin your GitOps adoption journey. Red Hat’s &lt;a href="https://developers.redhat.com/author/evan-shortiss/"&gt;Evan Shortiss&lt;/a&gt;, &lt;a href="https://developers.redhat.com/author/natale-vinto/"&gt;Natale Vinto&lt;/a&gt; (co-author of the new &lt;a href="https://developers.redhat.com/e-books/gitops-cookbook"&gt;GitOps Cookbook&lt;/a&gt;), and &lt;a href="https://developers.redhat.com/authors/ryan-jarvinen/"&gt;Ryan Jarvinen&lt;/a&gt; will demonstrate how to use OpenShift GitOps to customize and manage an OpenShift cluster. Attendees will also learn how to deploy and manage applications using Red Hat OpenShift GitOps, and how it can detect and auto-correct configuration drift between the desired configuration stored in a Git repository and the actual state on their OpenShift cluster. &lt;/p&gt; &lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1673389536296001y1dZ?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;From zero to hero in Kubernetes-native Java&lt;/h3&gt; &lt;p&gt;More than 16.5 million &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; developers are currently working to realize business requirements and spend a ton of time and effort to optimize the application performance for a variety of workloads (e.g., web, mobile, &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;AI/ML&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt;) in the cloud. A big challenge is adopting a new programming language or runtime over Java due to the lack of compatibility with Kubernetes. &lt;/p&gt; &lt;p&gt;In this session, Red Hat’s &lt;a href="https://developers.redhat.com/author/daniel-oh"&gt;Daniel Oh&lt;/a&gt; will walk you through how developers can scaffold a Java project from scratch and then evolve it as a Kubernetes-native application in terms of optimizing resources with the native build, integrating Kubernetes manifest (ConfigMap, Secret, Health Check), building container images, and deploying it to Kubernetes. You can also continue testing and debugging the application while it’s already deployed to the remote Kubernetes, the same as the local developer experiences of inner-loop development.&lt;/p&gt; &lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1671448348507001lKXA?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Vote App: A complete end-to-end story from code to prod&lt;/h3&gt; &lt;p&gt;In this lab, Red Hat’s &lt;a href="https://developers.redhat.com/author/natale-vinto/"&gt;Natale Vinto&lt;/a&gt;, &lt;a href="https://developers.redhat.com/author/cedric-clyburn/"&gt;Cedric Clyburn&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/author/evan-shortiss/"&gt;Evan Shortiss&lt;/a&gt; will walk through a complete development journey example of a 2-tier application made with &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;, built and deployed on Red Hat OpenShift. &lt;/p&gt; &lt;p&gt;You'll start coding with Red Hat OpenShift Dev Spaces, a modern in-browser IDE, and create container images with OpenShift Pipelines and Quay.io; then, you'll deploy and promote the application into several environments using OpenShift GitOps. This lab will help you get more familiar with cloud-native development, Kubernetes automation, and the GitOps methodology.&lt;/p&gt; &lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1673448136004001vsNU?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Enter Serverless Functions journey with Quarkus&lt;/h3&gt; &lt;p&gt;In this hands-on workshop, Red Hat’s &lt;a href="https://developers.redhat.com/author/daniel-oh"&gt;Daniel Oh&lt;/a&gt; walks through how to scaffold a &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt; functions project using Quarkus, a Kubernetes-native Java framework. He will cover how to deploy service functions to AWS Lambda, optimize the functions, and make them portable across multiple serverless platforms (e.g., AWS Lambda, Azure Functions, Google Cloud Platform, Kubernetes Knative). You’ll also use handy command-line tools like &lt;code&gt;kn func&lt;/code&gt; to enable a Buildpack for function development &amp; deployment in minutes. &lt;/p&gt; &lt;p&gt;Lab participants will be provided a free sandbox for serverless deployments. You’ll learn how to quickly create cloud-native microservice projects using Quarkus. Then, you’ll deploy the application to a function to AWS Lambda and Knative Event with JVM and Native mode.&lt;/p&gt; &lt;p&gt;&lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023/session/1671448706376001mamw?intcmp=7013a000003DMkYAAW"&gt;&lt;strong&gt;See full session details.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note: &lt;/strong&gt;Be sure to check back as more sessions are added here! In the meantime, you can &lt;a href="https://events.experiences.redhat.com/widget/redhat/sum23/SessionCatalog2023?tab.day=20230522"&gt;browse the full session catalog&lt;/a&gt; and &lt;a href="https://reg.experiences.redhat.com/flow/redhat/sum23/regGenAttendee/login?intcmp=7013a000003DMkYAAW"&gt;register now&lt;/a&gt; at the Red Hat Summit website. &lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/29/developers-guide-red-hat-summit-2023" title="The developer's guide to Red Hat Summit 2023"&gt;The developer's guide to Red Hat Summit 2023&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Colleen Lobner</dc:creator><dc:date>2023-03-29T14:00:00Z</dc:date></entry><entry><title>Deploy a Kafka Connect container using Strimzi</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/29/deploy-kafka-connect-container-using-strimzi" /><author><name>Alex Soto Bueno</name></author><id>81016972-b09b-4a16-8995-49bb71f4bd5b</id><updated>2023-03-29T07:00:00Z</updated><published>2023-03-29T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; are becoming de-facto platforms for developing and deploying &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservice&lt;/a&gt; architectures. One of the challenges you might face when using both technologies is deploying and managing Kafka brokers inside Kubernetes—dealing with YAMLs, management, &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; creation, etc.&lt;/p&gt; &lt;p&gt;The answer is &lt;a href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt;, an open source tool that lets you easily run Kafka clusters on Kubernetes in various deployment configurations. This article demonstrates how you can use Strimzi to configure Kafka Connect, a data integration framework for Kafka.&lt;/p&gt; &lt;h2&gt;Install the Strimzi Kubernetes Operator&lt;/h2&gt; &lt;p&gt;To start using Strimzi, install the Strimzi Kubernetes Operator to your cluster. You will need cluster-admin rights.&lt;/p&gt; &lt;p&gt;If you are using &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, the simplest way is to use the OperatorHub interface and search for &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;AMQ Streams&lt;/a&gt; (Strimzi for OpenShift), as shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_11.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image1_11.png?itok=IjFldCja" width="600" height="274" alt="Locating the AMQ Streams Operator by filtering results in OperatorHub." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Searching for AMQ Streams in OperatorHub.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Select the operator and push the &lt;strong&gt;Create&lt;/strong&gt; and &lt;strong&gt;Install&lt;/strong&gt; buttons (Figure 2).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image4_3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image4_3.png?itok=galtX6bM" width="600" height="286" alt="Installing the AMQ Streams Operator." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Installing the AMQ Streams Operator.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After the operator is installed, create a new OpenShift project named &lt;code&gt;kafka&lt;/code&gt; to deploy the Kafka brokers (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image2_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image2_5.png?itok=fkIHUJEz" width="600" height="155" alt="Creating a new project called kafka." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Creating a new OpenShift project to deploy the Kafka brokers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Install Strimzi using YAML&lt;/h3&gt; &lt;p&gt;Alternatively, you can install the Strimzi operator using a YAML file. This is necessary when using another Kubernetes distribution like &lt;a href="https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube"&gt;minikube&lt;/a&gt;. Run the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create namespace kafka&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Provision the Kafka cluster&lt;/h2&gt; &lt;p&gt;With the operator up and running, provision the Kafka cluster by applying a Kubernetes custom resource of kind Kafka.&lt;/p&gt; &lt;p&gt;In this example, we will deploy a Kafka cluster with a single node and ephemeral storage.&lt;/p&gt; &lt;p&gt;Create a new file named &lt;code&gt;kafka.yaml&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;apiVersion: kafka.strimzi.io/v1beta2 kind: Kafka metadata:   name: my-cluster spec:   kafka:     version: 3.2.3     replicas: 3     listeners:       - name: plain         port: 9092         type: internal         tls: false       - name: tls         port: 9093         type: internal         tls: true     config:       offsets.topic.replication.factor: 1       transaction.state.log.replication.factor: 1       transaction.state.log.min.isr: 1       default.replication.factor: 1       min.insync.replicas: 1       inter.broker.protocol.version: "3.2"     storage:       type: ephemeral   zookeeper:     replicas: 3     storage:       type: ephemeral   entityOperator:     topicOperator: {}     userOperator: {}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And apply the manifest:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl apply -f kafka.yaml -n kafka&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After a few minutes, Kafka is up and running in the Kubernetes cluster. One of the essential resources created, apart from the StatefulSet elements, is a Kubernetes service with the name &lt;code&gt;my-cluster&lt;/code&gt; (from the metadata name field set in the previous manifest). You need to use this hostname within the cluster to access Kafka.&lt;/p&gt; &lt;h2&gt;Create a new topic&lt;/h2&gt; &lt;p&gt;With the Kafka cluster ready, it’s time to create a new topic named &lt;code&gt;samples&lt;/code&gt; to publish events to.&lt;/p&gt; &lt;p&gt;Create a &lt;code&gt;topic.yaml&lt;/code&gt; file with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaTopic metadata:   name: samples   labels:     strimzi.io/cluster: my-cluster spec:   partitions: 1   replicas: 1   config:     retention.ms: 7200000     segment.bytes: 1073741824&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And apply it by running the &lt;code&gt;kubectl&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl apply -f topic.yaml -n kafka &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With that in place, you can start producing and consuming events. But let’s now focus on configuring &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Kafka Connect&lt;/a&gt; using Strimzi.&lt;/p&gt; &lt;h2&gt;Kafka Connect configuration&lt;/h2&gt; &lt;p&gt;Kafka Connect is an integration toolkit for streaming data between Kafka brokers and other systems. &lt;/p&gt; &lt;p&gt;Let’s assume the following scenario: A producer is generating events to the &lt;code&gt;samples&lt;/code&gt; topic. Each event has a JSON document payload with an &lt;code&gt;id&lt;/code&gt; and a &lt;code&gt;message&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;An example of the payload is shown in the snippet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;{   "message" : "Hi Duke",    "id" : "e2200c82-f1a0-4eb0-9e3c-74de800b5991"  }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We want to store these events in a &lt;a href="https://mongodb.com/"&gt;MongoDB&lt;/a&gt; database, as we need to consume them from a legacy application.&lt;/p&gt; &lt;p&gt;Moreover, to make things more interesting, the documents in MongoDB should get stored with &lt;code&gt;field welcome&lt;/code&gt; instead of &lt;code&gt;message&lt;/code&gt;, so you should rename the field before storing it inside the database.&lt;/p&gt; &lt;p&gt;One option could be to develop a consumer and producer receiving all events, transforming them, and storing them in the MongoDB instance. A better way is to use Kafka Connect with a MongoDB connector to automatically consume events and sink them to MongoDB without writing any code.&lt;/p&gt; &lt;p&gt;The diagram in Figure 4 summarizes the architecture using Kafka Connect.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image3_5.png?itok=orl3qs5R" width="600" height="173" alt="Diagram of the Kafka Connect architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The producer is a Quarkus application periodically sending messages to the &lt;code&gt;samples&lt;/code&gt; Kafka topic. The MongoDB &lt;a href="https://www.mongodb.com/docs/kafka-connector/current/sink-connector/"&gt;Kafka Connect Sink&lt;/a&gt; task will consume each message, make the adjustments and store it in the MongoDB collection.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Deploy the MongoDB instance with Helm&lt;/h3&gt; &lt;p&gt;Before deploying and configuring the Kafka Connect sink task, let’s deploy the MongoDB instance in the Kubernetes cluster using &lt;a href="https://helm.sh/"&gt;Helm&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In a terminal window, run the following command to register the &lt;a href="https://bitnami.com/stacks/helm"&gt;bitnami&lt;/a&gt; repo and install the &lt;a href="https://artifacthub.io/packages/helm/bitnami/mongodb"&gt;MongoDB Helm chart&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;helm repo add bitnami https://charts.bitnami.com/bitnami helm install mongodb bitnami/mongodb --set podSecurityContext.fsGroup="",containerSecurityContext.enabled=false,podSecurityContext.enabled=false,auth.enabled=false --version 13.6.0 -n kafka&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To deploy the producer, create a simple Deployment file with the following content and apply it to the Kubernetes cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--- apiVersion: apps/v1 kind: Deployment metadata:   annotations:     app.quarkus.io/build-timestamp: 2022-12-16 - 13:24:37 +0000   labels:     app.kubernetes.io/version: 1.0.0-SNAPSHOT     app.kubernetes.io/name: kafka-spam-messages   name: kafka-spam-messages spec:   replicas: 1   selector:     matchLabels:       app.kubernetes.io/version: 1.0.0-SNAPSHOT       app.kubernetes.io/name: kafka-spam-messages   template:     metadata:       annotations:         app.quarkus.io/build-timestamp: 2022-12-16 - 13:24:37 +0000       labels:         app.kubernetes.io/version: 1.0.0-SNAPSHOT         app.kubernetes.io/name: kafka-spam-messages     spec:       containers:         - env:             - name: KUBERNETES_NAMESPACE               valueFrom:                 fieldRef:                   fieldPath: metadata.namespace           image: quay.io/lordofthejars/kafka-spam-messages:1.0.0-SNAPSHOT           imagePullPolicy: Always           name: kafka-spam-messages&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl apply -f producer.yaml -n kafka&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point, the producer is generating periodic events to the &lt;code&gt;samples&lt;/code&gt; topic, but they are not transferred to MongoDB as the Kafka Connect sink task has yet to be deployed.&lt;/p&gt; &lt;h2&gt;Create and deploy a Kafka Connect container&lt;/h2&gt; &lt;p&gt;Apart from deploying a Kafka cluster, Strimzi also lets you create and deploy a Kafka Connect container.&lt;/p&gt; &lt;p&gt;Let’s create a new Strimzi CRD file of kind &lt;code&gt;KafkaConnect&lt;/code&gt; which will build a container image containing the MongoDB Kafka Connector Jar file downloaded from &lt;a href="https://maven.apache.org/pom.html#Maven_Coordinates"&gt;Maven coordinates&lt;/a&gt;. This is configured in the &lt;strong&gt;build&lt;/strong&gt; section of the CR.&lt;/p&gt; &lt;p&gt;Because the container image needs to be published into a protected container registry, you need to set a Kubernetes Secret name with the credentials to the container registry so you can push the image in the &lt;strong&gt;pushSecret&lt;/strong&gt; field.&lt;/p&gt; &lt;p&gt;Let’s start by creating a secret named &lt;code&gt;quayio&lt;/code&gt; with the container registry credentials from a Docker configuration file (you can run &lt;code&gt;docker login&lt;/code&gt; command to generate this file).&lt;/p&gt; &lt;p&gt;In a terminal window, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create secret generic quayio --from-file=.dockerconfigjson=/Users/asotobu/.docker/config.json --type=kubernetes.io/dockerconfigjson -n kafka&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;KafkaConnect&lt;/code&gt; file uses the &lt;code&gt;use-connector-resource&lt;/code&gt; annotation to let Strimzi configure the connector using the &lt;code&gt;KafkaConnector&lt;/code&gt; kind instead of the REST API.&lt;/p&gt; &lt;p&gt;Moreover, as mentioned before, Strimzi will build a new container image, push it, and finally deploy it. If you want to reuse an already created image, use the &lt;strong&gt;image&lt;/strong&gt; field and remove the &lt;strong&gt;build&lt;/strong&gt; field in the KafkaConnect custom resource to skip the build phase and deploy it directly.&lt;/p&gt; &lt;p&gt;Create a new file with the name &lt;code&gt;mongodb-kc.yaml&lt;/code&gt;with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaConnect metadata:   name: mongodb-sink-connect-cluster   annotations:     strimzi.io/use-connector-resources: "true" spec:   version: 3.2.3   replicas: 1   bootstrapServers: my-cluster-kafka-bootstrap:9092   # image: quay.io/lordofthejars/mongodb-sink-connect-cluster:latest    build:     output:       type: docker       image: quay.io/lordofthejars/mongodb-sink-connect-cluster:latest       pushSecret: quayio      plugins:       - name: my-plugin         artifacts:           - type: maven             repository: https://repo1.maven.org/maven2              group: org.mongodb.kafka             artifact: mongo-kafka-connect             version: 1.8.1   config:     group.id: connect-cluster     key.converter: org.apache.kafka.connect.json.JsonConverter     value.converter: org.apache.kafka.connect.json.JsonConverter     key.converter.schemas.enable: false     value.converter.schemas.enable: false     offset.storage.topic: connect-offsets     config.storage.topic: connect-configs     status.storage.topic: connect-status&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Apply the file in a terminal using &lt;code&gt;kubectl apply&lt;/code&gt; in the kafka namespace command and wait until the task is deployed; this might take a few minutes as it needs to create a container, push it, and finally deploy it.&lt;/p&gt; &lt;h2&gt;Configure the MongoDB sink connector&lt;/h2&gt; &lt;p&gt;The final step is configuring the MongoDB sink connector. The following configuration parameters are set:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The &lt;code&gt;strimzi.io/cluster&lt;/code&gt; label needs to be set to the &lt;code&gt;KafkaConnect&lt;/code&gt; name value so Strimzi can configure the instance correctly.&lt;/li&gt; &lt;li aria-level="1"&gt;The &lt;code&gt;topics&lt;/code&gt; where events are consumed.&lt;/li&gt; &lt;li aria-level="1"&gt;The location of the MongoDB instance (&lt;code&gt;connection.uri&lt;/code&gt;) as well as the database (&lt;code&gt;database&lt;/code&gt;) and collection (&lt;code&gt;collection&lt;/code&gt;) where events are stored.&lt;/li&gt; &lt;li aria-level="1"&gt;A rename &lt;code&gt;transformer&lt;/code&gt; changing the JSON payload, concretely renaming the field &lt;code&gt;message&lt;/code&gt; to &lt;code&gt;welcome&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Create a new file with the name &lt;code&gt;mongodb-kcn.yaml &lt;/code&gt;with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaConnector metadata:   name: connector-mongodb-sink   labels:     strimzi.io/cluster: mongodb-sink-connect-cluster spec:   class: com.mongodb.kafka.connect.MongoSinkConnector   tasksMax: 1   config:     topics: samples     key.converter: org.apache.kafka.connect.storage.StringConverter      value.converter: org.apache.kafka.connect.json.JsonConverter     value.converter.schemas.enable: false     connection.uri: mongodb://mongodb:27017     database: sampledb     collection: samples     transforms: rename     transforms.rename.type: "org.apache.kafka.connect.transforms.ReplaceField$Value"     transforms.rename.renames: "message:welcome"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Apply the previous manifest to start populating events from the Kafka topic to the MongoDB collection without writing any code and all the power of Kafka.&lt;/p&gt; &lt;p&gt;Run the following command to apply the previous manifest:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl apply -f mongodb-kcn.yaml -n kafka&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Validate the MongoDB content&lt;/h2&gt; &lt;p&gt;To validate that the example works correctly, let’s query the MongoDB content using the &lt;code&gt;mongo&lt;/code&gt; CLI tool.&lt;/p&gt; &lt;p&gt;Execute the following command to start a container image within the Kubernetes cluster with the &lt;code&gt;mongo&lt;/code&gt; CLI tool installed:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl run --namespace kafka mongodb-client --rm --tty -i --restart='Never' --image docker.io/bitnami/mongodb:4.4.13-debian-10-r9 --command -- bash&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When Pod is up and running, start the MongoDB shell: &lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mongo mongodb://mongodb:27017&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Within the MongoDB shell, select the &lt;code&gt;sampledb&lt;/code&gt; database to query:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;use sampledb&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And query the &lt;code&gt;samples&lt;/code&gt; collection:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;db.samples.find();&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You’ll get the list of documents inserted by MongoDB Kafka Connect task:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;{ "_id" : ObjectId("63bc2eebb2ff5965a0446336"), "welcome" : "Bonjour", "id" : "d2b67d9f-cbee-4028-8443-cd4f2117d0ce" } { "_id" : ObjectId("63bc2eebb2ff5965a0446337"), "welcome" : "Bonjour", "id" : "60b8b33d-3726-43df-9597-276652185e93" } { "_id" : ObjectId("63bc2eebb2ff5965a0446338"), "welcome" : "Good Bye Cruel World", "id" : "8432d1ea-a6bc-4bcc-9ca7-45657ebb3d89" }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The bottom line&lt;/h2&gt; &lt;p&gt;Kafka and its ecosystem are practical tools for solving many problems these days in the (micro)services era. As we’ve seen, Kafka Connect greatly helps us integrate data systems, providing a secure, scalable, and easy way to connect systems.&lt;/p&gt; &lt;p&gt;But adopting them in Kubernetes is not trivial. Strimzi is a perfect choice to solve these problems and create a smooth experience when running Kafka and Kafka Connect in Kubernetes.&lt;/p&gt; &lt;p&gt;Explore more &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;tutorials from Red Hat Developer for running Apache Kafka on Kubernetes&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/29/deploy-kafka-connect-container-using-strimzi" title="Deploy a Kafka Connect container using Strimzi"&gt;Deploy a Kafka Connect container using Strimzi&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alex Soto Bueno</dc:creator><dc:date>2023-03-29T07:00:00Z</dc:date></entry><entry><title type="html">Update on deprecation of Keycloak adapters</title><link rel="alternate" href="https://www.keycloak.org/2023/03/adapter-deprecation-update" /><author><name>Stian Thorgersen</name></author><id>https://www.keycloak.org/2023/03/adapter-deprecation-update</id><updated>2023-03-29T00:00:00Z</updated><content type="html">In 2022 we announced the deprecation of , with a plan to stop delivering most adapters in . As we have not been able to make sufficient progress on finding alternatives and work on supporting material to help migrating away from Keycloak adapters we are extending the life of the Keycloak adapters. The plan is still to eventually stop delivering bespoke Keycloak adapters in the future, but we will do this in a more gradual process than previous laid out. We still strongly belive that the community as a whole are better served in the long run by us focusing more on the Keycloak server with full compliance and support for specifications such as OAuth 2.0 and OpenID Connect, and adding support for additional relevant extensions to the specifications. We also believe by leaving the integration for various programming languages and frameworks to the relevant communities, the end result will be more extensive support, with more features and abilities, and last but not least better integrations and easy of use. OAUTH 2.0 AND OPENID CONNECT ADAPTERS JAVA For Java applications there is now more than ever wide-spread support for OpenID Connect, where some examples include: * - OpenID Connect support in Jakarta EE 10 * - OpenID Connect support in WildFly * - OpenID Connect support for Quarkus applications * - OAuth and OpenID Connect support in Spring * - The Java security framework to protect all your web applications and web services Neither of these have support for Keycloak Authorization Services though, which is why we are planning to introduce a generic Java client libraries for Authorization Services that can be leveraged with other OpenID Connect client libraries. Expect this to be delivered in Keycloak 22. The Keycloak Java adapters will remain for a while though, at least towards the end of the year, but likely not be removed until early 2024. At the same time don’t expect the adapters to be updated in terms of adding new features, enhancements, or supporting newer versions of Tomcat, Jetty, WildFly, or Spring. NODE.JS We are still investigating alternatives for Node.js, so plan is available for those one just yet. Expect more information to come later in the year. Regardless of the alternative we will deliver support for Keycloak Authorization Services to Node.js. The Keycloak Node.js adapter will remain, at least towards the end of the year, but likely not be removed until early 2024. CLIENT-SIDE JAVASCRIPT For now the Keycloak client-side JavaScript adapter remains, but we are looking into alternatives as well as the potential of completely overhauling our current adapter and continue maintaining and delivering this adapter. SAML 2.0 We are planning to continue supporting SAML 2.0 for WildFly and JBoss EAP in the long run, but support for Tomcat and Jetty are likely to be removed relatively soon.</content><dc:creator>Stian Thorgersen</dc:creator></entry><entry><title>How to configure SOAP web services with Apache Camel</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/28/how-configure-soap-web-services-apache-camel" /><author><name>Michael Thirion</name></author><id>312bb7ef-e872-4a11-9b54-6ff82cc74b8c</id><updated>2023-03-28T07:00:00Z</updated><published>2023-03-28T07:00:00Z</published><summary type="html">&lt;p&gt;This article demonstrates how to configure Simple Object Access Protocol (SOAP) web services with the Red Hat build of Apache Camel, &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; version. In Apache Camel version 3, the support for the SOAP protocol is still provided by the CXF framework. Therefore, on Quarkus, we will be relying on the camel-quarkus-cxf-soap extension.&lt;/p&gt; &lt;h2&gt;A common REST to SOAP transformation use case&lt;/h2&gt; &lt;p&gt;With the CXF runtime, there is a distinction to make between a SOAP service and the client of a SOAP service.&lt;/p&gt; &lt;p&gt;Let's use a very common use case to demonstrate those two parts distinctively. Let's imagine a legacy SOAP web service backend that we want to expose behind a REST endpoint (Figure 1).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_9.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/arch_9.png?itok=xuf29oUi" width="497" height="122" alt="Use case architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1. Use case architecture.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;You can find the source code of the two applications developed for that purpose on this &lt;a href="https://github.com/mthirion/camel-quarkus-cxf-soap-and-ws-security"&gt;GitHub page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The SOAP web service backend is represented by a mock implementation of a publicly available definition, sourced from &lt;a href="https://www.herongyang.com/1000230_Live_Examples_of_Web_Services.html"&gt;herongyang.com&lt;/a&gt;. The web service at play is Registration SOAP 1.1.&lt;/p&gt; &lt;p&gt;For both the CXF client and server development, the first step is to generate the &lt;a href="http://developers.redhat.com/topics/java"&gt;Java&lt;/a&gt; objects corresponding to the WSDL elements. This can be done with the cxf-codegen-plugin Maven plugin, whose wsdl2java task generates Java representations of the soap request and response payloads, as well as the service interface and its implementation (PortType).&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ tree target/generated-sources/cxf target/generated-sources/cxf └── https └── www_herongyang_com └── service ├── ObjectFactory.java ├── package-info.java ├── RegistrationPortType.java ├── RegistrationRequest.java ├── RegistrationResponse.java └── RegistrationService.java&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configuring a Camel CXF SOAP server&lt;/h2&gt; &lt;p&gt;Let's now dig into the core part of the configuration.&lt;/p&gt; &lt;p&gt;On the server side, there is a Camel route that starts with the following line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;from("cxf:registration?serviceClass=https.www_herongyang_com.service.RegistrationPortType")&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second element of the endpoint URI is the name of a CxfEndpoint Java bean that contains the configuration of the CXF endpoint such as the service name, endpoint name, and exposed URL.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@Named("registration") @ApplicationScoped public class SoapServiceBean extends CxfEndpoint { public SoapServiceBean() { super(); try { this.configure(); } catch (ClassNotFoundException e) { e.printStackTrace(); } } private void configure() throws ClassNotFoundException { QName serviceQname = new QName("https://www.herongyang.com/Service/", "registrationService"); // From WSDL :: &lt;wsdl:service name="registrationService"&gt; this.setServiceNameAsQName(serviceQname); QName endpointQname = new QName("https://www.herongyang.com/Service/", "registrationPort"); // From WSDL :: &lt;wsdl:port name="registrationPort"&gt; this.setEndpointNameAsQName(endpointQname); this.setAddress("http://localhost:8055/soap/registration"); // Exposed address with the /soap root context defined in application.properties } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's it!&lt;/p&gt; &lt;p&gt;With that configuration, the web service should be available at &lt;code&gt;http://localhost:8085/soap/registration&lt;/code&gt;. We can test it with SOAPUI using the following request:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/" xmlns:ser="https://www.herongyang.com/Service/"&gt; &lt;&lt;soap:Header/&gt;&gt; &lt;&lt;soap:Body&gt;&gt; &lt;ser:RegistrationRequest date="now" event="123"&gt; &lt;Guest&gt;John&lt;/Guest&gt; &lt;/ser:RegistrationRequest&gt; &lt;/soap:Body&gt; &lt;/soap:Envelope&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configuring a Camel CXF SOAP client&lt;/h2&gt; &lt;p&gt;In the case of a client, the CxfEndpoint bean is not required. The URI can directly embed the target address as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; .to("cxf:http://localhost:8055/soap/registration?serviceClass=https.www_herongyang_com.service.RegistrationPortType")&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can attempt an end-to-end test with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -H "Content-Type: application/json" -XPOST http://localhost:8050/api/registration -d '{"event":"123","guest":"john"}'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Applying WS-Security or other SOAP features&lt;/h2&gt; &lt;p&gt;The CXF framework still builds on the concept of interceptors and interceptors chain. On Quarkus, interceptors are injected into the CXF runtime in Java.  The hook to the CXF runtime is the cxfConfigurer option of the URI endpoint.&lt;/p&gt; &lt;p&gt;For the server, the endpoint URI becomes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;from("cxf:registration?serviceClass=https.www_herongyang_com.service.RegistrationPortType&amp;cxfConfigurer=#mywssecurity")&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On the other hand, for the client, it becomes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;to("cxf:http://localhost:8055/soap/registration?serviceClass=https.www_herongyang_com.service.RegistrationPortType&amp;cxfConfigurer=#mywssecurity")&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On both lines, the &lt;code&gt;mywssecurity&lt;/code&gt; element is a reference to a Java bean that implements the CxfConfigurer interface. This interface has methods to configure either the CXF server, client, or both. Thus, in our case, on the client side, we have:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@Named("mywssecurity") @ApplicationScoped public class WSServiceBean implements CxfConfigurer { @Inject @ConfigProperty (name = "app.webservice.soap.wssecurity.user") // custom property defined in application.properties private String user; @Inject @ConfigProperty (name = "app.webservice.soap.wssecurity.mustunderstand") // custom property defined in application.properties private String mustunderstand; public WSServiceBean() { super(); } @Override public void configure(AbstractWSDLBasedEndpointFactory factoryBean) { // TODO Auto-generated method stub //throw new UnsupportedOperationException("Unimplemented method 'configure'"); } @Override public void configureClient(Client client) { Map&lt;String,Object&gt; wsproperties = new HashMap&lt;String, Object&gt;(); wsproperties.put(ConfigurationConstants.ACTION, ConfigurationConstants.USERNAME_TOKEN_NO_PASSWORD); wsproperties.put(ConfigurationConstants.ALLOW_USERNAMETOKEN_NOPASSWORD, "true"); wsproperties.put(ConfigurationConstants.USER, user); wsproperties.put(ConfigurationConstants.MUST_UNDERSTAND, mustunderstand); WSS4JOutInterceptor wssecurity = new WSS4JOutInterceptor(wsproperties); client.getOutInterceptors().add(wssecurity); //throw new UnsupportedOperationException("Unimplemented method 'configureClient'"); } @Override public void configureServer(Server server) { // TODO Auto-generated method stub //throw new UnsupportedOperationException("Unimplemented method 'configureServer'"); } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;While on the server side, we have the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@Named("mywssecurity") @ApplicationScoped public class WSServiceBean implements CxfConfigurer { @Inject @ConfigProperty (name = "app.webservice.soap.wssecurity.user") // custom property defined in application.properties private String user; public WSServiceBean() { super(); } @Override public void configure(AbstractWSDLBasedEndpointFactory factoryBean) { // TODO Auto-generated method stub //throw new UnsupportedOperationException("Unimplemented method 'configure'"); } @Override public void configureClient(Client client) { // TODO Auto-generated method stub //throw new UnsupportedOperationException("Unimplemented method 'configureClient'"); } @Override public void configureServer(Server server) { Map&lt;String,Object&gt; wsproperties = new HashMap&lt;String, Object&gt;(); wsproperties.put(ConfigurationConstants.ACTION, ConfigurationConstants.USERNAME_TOKEN_NO_PASSWORD); wsproperties.put(ConfigurationConstants.ALLOW_USERNAMETOKEN_NOPASSWORD, "true"); wsproperties.put(ConfigurationConstants.USER, user); WSS4JInInterceptor wssecurity = new WSS4JInInterceptor(wsproperties); server.getEndpoint().getInInterceptors().add(wssecurity); //throw new UnsupportedOperationException("Unimplemented method 'configureClient'"); } }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article demonstrates how to configure SOAP web services with the Red Hat build of Apache Camel, Quarkus version. If you have questions, feel free to comment below. We welcome your feedback!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/28/how-configure-soap-web-services-apache-camel" title="How to configure SOAP web services with Apache Camel"&gt;How to configure SOAP web services with Apache Camel&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Thirion</dc:creator><dc:date>2023-03-28T07:00:00Z</dc:date></entry><entry><title type="html">How to configure CORS on WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/web/jboss-web-server/how-to-configure-cors-on-wildfly/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/web/jboss-web-server/how-to-configure-cors-on-wildfly/</id><updated>2023-03-27T19:57:30Z</updated><content type="html">If you are running your web applications on WildFly, you can configure CORS to allow cross-domain requests. In this article, we will go through the steps to configure CORS on WildFly. What is a Cross Resource Sharing (CORS)? Cross-Origin Resource Sharing (CORS) is a security mechanism that allows web applications to make requests to a ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to tune the performance of Java 11 HttpClients</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-tune-the-performance-of-java-11-httpclients/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-tune-the-performance-of-java-11-httpclients/</id><updated>2023-03-27T11:08:41Z</updated><content type="html">The Java 11 HttpClient API provides a lot of configuration options that you can use to tune its performance. In this tutorial, we will explore some of the most important ones. Connection Pooling Connection pooling is a technique that allows you to reuse existing connections instead of creating new ones for each request. This can ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Writing high performance Apache HTTP Clients</title><link rel="alternate" href="http://www.mastertheboss.com/java/writing-high-performance-java-http-client-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/writing-high-performance-java-http-client-applications/</id><updated>2023-03-27T08:40:59Z</updated><content type="html">When it comes to consuming HTTP resources in Java applications, Apache HTTP Client is a popular choice for developers due to its ease of use, flexibility, and robustness. In this article, we will explore how to write a high-performance Java HTTP client using the Apache HTTP Client library. Disambiguation: This article discusses about the performance ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
